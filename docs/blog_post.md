# From Random to Contextual: Comparing Bandit Algorithms for News Recommendation

## Motivation

Every news website faces the same problem: a user arrives, the system has a handful of candidate articles, and it must decide which one to show. If the user clicks, that's a success. If not, an opportunity was wasted. The goal is straightforward — maximize the click-through rate (CTR) over time — but the path to getting there is not.

The core challenge is the **explore/exploit dilemma**. Should the system recommend the article it currently believes is best (exploit), or should it try something less certain to learn more about what works (explore)? Pure exploitation misses potentially great articles it has never tried. Pure exploration wastes impressions on articles that have already proven ineffective. Every recommendation system must navigate this tradeoff.

Traditional A/B testing is one approach: split users into groups, show each group a different article, and after collecting enough data, pick the winner. But A/B tests are static. They require pre-defined variants, run to completion before any learning happens, and start over from scratch with each new test. In a domain like news — where content changes hourly and user preferences shift — this rigidity is costly.

**Bandit algorithms** offer a better model. They learn in real time, continuously adapting their recommendations based on observed feedback. Instead of waiting for a test to finish, a bandit shifts traffic toward better-performing options while still exploring alternatives. The learning never stops.

Within the bandit framework there is a spectrum of sophistication. **Multi-armed bandits** (MABs) treat every user the same — they learn which articles are globally popular but cannot personalize. **Contextual bandits** go further, using information about the user and the article to make personalized decisions. A sports fan and a finance reader should see different recommendations, and contextual algorithms can learn these preferences.

In this post we compare four algorithms that span this spectrum, from a random baseline to a fully contextual approach. We evaluate them head-to-head on the Microsoft News Dataset (MIND), a collection of real user impression logs. The question: how much does each level of sophistication actually buy you?

## Methods

All four algorithms face the same decision each round: given a set of candidate articles, pick one. After the user either clicks or ignores the recommendation, the algorithm updates its internal state. The difference between algorithms is *how* they make their choice and *what* they learn from the outcome.

### RandomChoice (Baseline)

The simplest possible strategy: pick an article uniformly at random with no regard for past performance, user history, or article content.

**Strengths:** Dead simple and completely unbiased. It makes no assumptions and requires no tuning. Its only purpose is to establish a performance floor — any useful algorithm must beat this.

**Weaknesses:** It ignores everything. Every click it gets is pure luck. With ~37 candidates per round, we'd expect a CTR around 3%, but since some articles are more popular than others the actual random CTR lands around 11%.

### Epsilon-Greedy

Epsilon-greedy introduces the most basic form of learning. It tracks the average reward (click rate) for each article it has tried. With probability 1-ε it *exploits* by picking the article with the highest known average. With probability ε it *explores* by picking randomly. We use ε=0.1, meaning it exploits 90% of the time.

**Strengths:** Easy to understand and implement. It learns quickly which articles are popular across the user base. The ε parameter gives direct, intuitive control over the explore/exploit balance — turn it up for more exploration, down for more exploitation.

**Weaknesses:** Its exploration is blind. When it does explore, it picks randomly among all candidates, including ones that have already proven poor. It also treats all users identically — a sports fan and a foodie see the same recommendations. And it never stops exploring, even when it is highly confident about which articles are best. That 10% random exploration continues forever, wasting impressions on known losers.

### Thompson Sampling

Thompson Sampling takes a Bayesian approach to the explore/exploit problem. Instead of tracking a single average per article, it maintains a full probability distribution (a Beta distribution) over each article's true click rate. At decision time, it draws a random sample from each article's distribution and picks the article whose sample is highest.

**Strengths:** Exploration is principled and automatic. Articles that the algorithm is uncertain about have wide distributions — their samples sometimes come out high, naturally triggering exploration. Articles with strong evidence of high click rates have narrow distributions centered on good values, so they consistently win. There is no exploration parameter to tune. As confidence grows, exploration naturally tapers off.

**Weaknesses:** Like epsilon-greedy, Thompson Sampling is context-free. It cannot personalize. It is also sensitive to the choice of prior distribution. With over 18,000 unique articles appearing across 100,000 impressions, many articles are seen only a handful of times. A uniform prior — Beta(1,1), expressing total ignorance — can slow down learning when the arm space is large. A pessimistic prior calibrated to the base click rate helps the algorithm exploit observed clicks sooner.

**Key insight:** We use a prior of Beta(1,8), which encodes a prior belief that articles are clicked about 11% of the time (close to the actual base rate). This lets even a single click on an article meaningfully shift its posterior upward. Prior tuning is a practical consideration that textbooks often gloss over, but it matters in real-world settings with large article catalogs.

### LinUCB (Contextual Bandit)

LinUCB, introduced by Li et al. at WWW 2010, is a contextual bandit algorithm. Unlike the previous approaches, it does not treat articles or users as interchangeable. Instead, it builds a linear model for each article that predicts click probability from a **context vector** — a numerical description of the user and the article.

At decision time, LinUCB computes a predicted reward for each candidate *plus* an "optimism bonus" that is large when the algorithm is uncertain about a prediction. It picks the article with the highest optimistic estimate. This is the Upper Confidence Bound (UCB) principle: be optimistic in the face of uncertainty, because the upside of discovering a great option outweighs the cost of a bad trial.

The context vector we use is 62-dimensional, built from subcategory information rather than the coarser top-level categories. The MIND dataset has 285 subcategories — far more granular than the 18 categories. We keep the top 30 most frequent subcategories (covering 84% of articles) plus an "other" bucket, giving a 31-dimensional one-hot per article. The first 31 dimensions encode the **user profile**: a normalized subcategory-frequency vector from the user's click history. A user who clicked mostly "football_nfl" and "basketball_nba" articles would have high weight on those subcategories. The remaining 31 dimensions encode the **article subcategory** as a one-hot. This lets LinUCB learn fine-grained patterns like "NFL fans tend to click NFL articles" rather than just "sports fans like sports."

**Strengths:** LinUCB personalizes. Different users get different recommendations based on their reading history. It shares learning across contexts — a click on a sports article by a sports-oriented user informs predictions for similar user-article pairs, not just that specific article. And its exploration bonus shrinks automatically as confidence grows, so it explores less over time without requiring manual tuning.

**Weaknesses:** LinUCB is only as good as the features you give it. The algorithm itself is elegant, but feature engineering is the real bottleneck.

**Key insight:** Feature granularity matters. With only 18 top-level categories, LinUCB was learning "which categories get clicked globally" — something epsilon-greedy already captures by tracking per-article averages. Switching to 285 subcategories (bucketed into top-30 + other) and adding user subcategory profiles from click history meaningfully improved LinUCB's performance. The algorithm needs fine-grained user-article interaction signals to personalize effectively.

## Experiment

### Dataset

We evaluate on MINDlarge_train, the large training split from the Microsoft News Dataset (MIND). The full dataset contains over 2.2 million impression logs and 101,527 news articles. We sample the first 100,000 impressions for our comparison, which covers 18,866 unique articles across 18 categories including news, sports, entertainment, finance, lifestyle, health, and more. Each impression presents approximately 37 candidate articles on average, with ground-truth click labels indicating which article the user actually engaged with.

### Simulation

We replay impressions sequentially through a simulation engine. For each round, the algorithm sees the candidate articles (and, for LinUCB, the context vectors), selects one article to recommend, and then observes the ground-truth reward: 1.0 if the user clicked that article, 0.0 otherwise. The algorithm updates its internal state and moves to the next round.

Critically, all four algorithms process the same 100,000 impressions in the same order. This ensures a fair apples-to-apples comparison — differences in CTR reflect algorithmic merit, not data ordering.

### Context Construction

For LinUCB, we construct a 62-dimensional context vector per candidate article in each round. Rather than using the 18 top-level categories, we use subcategory information — the MIND dataset has 285 subcategories like "football_nfl", "newsus", and "lifestylebuzz". We keep the top 30 most frequent subcategories (covering 84% of articles) plus an "other" bucket, giving 31 dimensions. The first 31 dimensions encode the **user profile**: a normalized subcategory-frequency vector built from the articles the user has previously clicked. A user who clicked 3 NFL football and 1 NBA basketball article would have high weight on those specific subcategories. The remaining 31 dimensions encode the **article subcategory** as a one-hot vector. This concatenated representation gives LinUCB fine-grained user-article interaction signals for personalization.

### Metric

Our primary metric is click-through rate (CTR): total clicks divided by total impressions. We also track cumulative CTR at each round to visualize how algorithms learn over time.

## Results

### Final Performance

| Algorithm | CTR | vs. Random |
|---|---|---|
| RandomChoice | 0.1088 | — |
| EpsilonGreedy | 0.1931 | +77.5% |
| ThompsonSampling | 0.1821 | +67.4% |
| LinUCB | 0.1789 | +64.4% |

![Final CTR comparison across all four algorithms](../results/final_ctr.png)

All three learning algorithms substantially outperform the random baseline. Epsilon-greedy leads with a 77% relative improvement, followed by Thompson Sampling and LinUCB close behind. With 100,000 impressions, the differences between algorithms are clearly visible — EpsilonGreedy achieves 19,313 clicks compared to RandomChoice's 10,876, and LinUCB nearly matches Thompson Sampling at 17,893 clicks.

### Learning Curves

![Cumulative CTR over 100,000 impression rounds](../results/cumulative_ctr.png)

The cumulative CTR plot reveals how each algorithm learns over time. RandomChoice converges to ~10.9% and stays flat — there is no learning happening. EpsilonGreedy ramps up fastest, reaching near its final CTR by around 5,000 rounds. Thompson Sampling follows a similar trajectory but stabilizes slightly lower. LinUCB starts slower — its linear models need more data to learn the relationship between subcategory features and clicks — but steadily climbs throughout the entire run, closing the gap with Thompson Sampling by round 100,000 and still trending upward. All three learning algorithms clearly separate from the random baseline within the first few thousand rounds.

### Takeaways

**1. Any learning beats no learning.** Even the simplest learning algorithm, epsilon-greedy, achieves a 77% relative improvement over random. With 100,000 impressions, that translates to over 8,400 additional clicks. The explore/exploit framework delivers real, measurable value.

**2. Simple algorithms are hard to beat.** Epsilon-greedy — the most straightforward learning approach — outperforms both Thompson Sampling and LinUCB on this dataset. Its fast exploitation of per-article averages is highly effective when the same articles recur across many impressions. Sophistication does not automatically mean better performance.

**3. Bayesian priors matter.** Thompson Sampling performs well with a Beta(1,8) prior calibrated to the base click rate. Prior tuning is a practical consideration that textbooks often gloss over, but it matters in real-world settings with large article catalogs where many arms are sparsely observed.

**4. Feature granularity unlocks contextual algorithms.** Switching from 18 top-level categories to the top-30 subcategories meaningfully improved LinUCB's performance. Subcategories like "football_nfl" and "basketball_nba" carry far more preference signal than a flat "sports" label. LinUCB's learning curve is still climbing at 100,000 rounds, suggesting that with more data — or even richer features like title embeddings — it could surpass the context-free approaches.

**5. The framework matters more than any single result.** A clean algorithmic interface — select an arm, observe a reward, update — makes it trivial to swap in new approaches. The engineering investment in a modular simulation framework pays dividends long after any individual experiment.

## Beyond News: Bandits for LLM Prompt Shaping

The news recommendation problem has a clean structure — pick an article, observe a click — but the bandit framework is far more general. Any setting where a system must choose an action, observe feedback, and improve over time is a bandit problem. One compelling application is **prompt shaping** for large language models (LLMs).

### The Problem

LLMs are sensitive to how they are prompted. The same underlying question can produce dramatically different responses depending on the system prompt, tone instructions, or framing that precedes it. Today, most LLM applications use a single static system prompt chosen by the developer through manual experimentation. This is the equivalent of A/B testing in the news domain — rigid, slow to adapt, and blind to individual user differences.

What if the system could learn, in real time, which prompting strategy works best for each user?

### Shaping Prompts as Arms

The bandit formulation maps naturally onto this problem. Define a fixed set of **shaping prompts** — short directives prepended to the LLM's system prompt that steer its behavior along a specific axis. Each shaping prompt is an arm. The system selects one before generating a response, observes user feedback (explicit ratings, continued engagement, follow-up tone), and updates its beliefs about which shaping prompt works best.

Consider a conversational assistant with the following shaping prompts:

| Arm | Shaping Prompt | Behavioral Effect |
|---|---|---|
| A | "Validate the user's perspective before responding." | Agreeable, supportive tone |
| B | "Respectfully challenge assumptions in the user's message." | Contrarian, Socratic tone |
| C | "Respond with concise, factual statements only." | Neutral, informational tone |
| D | "Mirror the user's energy and emotional register." | Adaptive, empathetic tone |

A multi-armed bandit would learn which of these four strategies produces the best engagement *on average across all users*. Maybe Arm D wins globally because most people prefer responses that match their energy. But this ignores individual differences — a user who is venting may want validation (Arm A), while a user stress-testing an idea may prefer pushback (Arm B).

### Why Contextual Bandits Fit

This is where contextual bandits shine. Just as LinUCB uses subcategory features to personalize article recommendations, a contextual bandit can use **conversational context features** to personalize prompt selection. The context vector might encode:

- **Tone of the user's recent messages** — sentiment polarity, assertiveness, question density
- **Conversation trajectory** — is the user escalating, de-escalating, or steady?
- **Topic signals** — technical question vs. emotional discussion vs. creative brainstorming
- **Session history** — which shaping prompts have already been tried and how the user responded

With these features, LinUCB (or a similar contextual algorithm) could learn patterns like:

- Users with high-assertiveness messages respond better to Arm B (challenge assumptions) — they want intellectual sparring, not agreement.
- Users with high-negative-sentiment messages respond better to Arm A (validate first) — they need to feel heard before engaging with substance.
- Users asking rapid-fire factual questions respond best to Arm C (concise facts) — they want efficiency, not empathy.

These are exactly the kinds of fine-grained interaction patterns that context-free algorithms cannot capture. Epsilon-greedy would converge on a single globally popular strategy and miss the personalization opportunity entirely.

### Agree or Disagree: A Concrete Example

To make this tangible, consider the simplest version: two arms, **agree** and **disagree**. The shaping prompt either tells the LLM to affirm the user's position or to push back on it. The context vector encodes the tone of the user's previous message — a single feature capturing sentiment on a scale from strongly negative to strongly positive.

A contextual bandit could learn the following policy:

- When the user's tone is **frustrated or negative** → agree (validate their experience, then gently reframe)
- When the user's tone is **confident or assertive** → disagree (introduce a counterpoint they may not have considered)
- When the user's tone is **neutral or exploratory** → either works, so explore more

This is a learned policy, not a hardcoded rule. The bandit discovers it from reward signals — perhaps measured by whether the user continues the conversation, asks a follow-up, or rates the response positively. And because it is learned online, it adapts as user behavior shifts. If a user who normally prefers validation starts asking for critical feedback, the bandit picks up on the context change and adjusts.

### Design Considerations

Applying bandits to prompt shaping introduces considerations that differ from the news recommendation setting:

**Reward signal.** In news, the reward is binary and immediate — did the user click? In a conversation, reward is harder to define. Possible signals include explicit thumbs-up/down ratings, whether the user continues the conversation, response length as a proxy for engagement, or sentiment shift between consecutive user messages. Each has tradeoffs between noise and informativeness.

**Arm set size.** The news setting has thousands of articles; prompt shaping works best with a small, curated set of arms (4–8 shaping prompts). This is an advantage — fewer arms means faster convergence and less wasted exploration. But it puts pressure on the prompt designer to choose a diverse, meaningful set of behavioral axes.

**Stationarity.** User preferences in a conversation can shift within a single session. A user might start exploratory and become frustrated. The bandit must respond to these within-session shifts, which favors algorithms with short memory or context features that capture trajectory, not just the current state.

**Composability.** Shaping prompts need not be mutually exclusive. A richer formulation might treat each behavioral axis as an independent bandit — one for tone (supportive vs. challenging), one for verbosity (concise vs. detailed), one for structure (freeform vs. bulleted). This combinatorial approach scales the action space but allows finer-grained personalization.

### Connection to the Simulation Framework

The modular architecture from this project — abstract algorithm interface, simulation engine, pluggable reward signals — transfers directly to the prompt shaping domain. The `BanditAlgorithm` interface (select an arm, observe a reward, update) does not care whether arms are news articles or shaping prompts. A prompt-shaping simulation could reuse the same engine with a different data loader that replays conversation logs instead of impression logs, and a different reward function that scores LLM responses instead of recording clicks.

The key insight from our news recommendation results also carries over: **feature granularity unlocks contextual algorithms.** Coarse features like "positive vs. negative tone" will produce modest personalization gains — analogous to LinUCB with only 18 categories. Fine-grained features capturing specific conversational patterns — analogous to the 285 subcategories — are where contextual bandits can meaningfully outperform simpler approaches.

### What's Next

Natural extensions of this work include enriching features with title embeddings for semantic similarity, running on the full 2.2 million impressions to see if LinUCB's upward trajectory overtakes the context-free algorithms, adding algorithms like UCB1 and contextual Thompson Sampling, and running statistical significance tests to quantify the reliability of observed differences. The prompt-shaping application outlined above offers another direction entirely — adapting the same bandit framework to a domain where the action space is small but the context is rich, and where personalization can meaningfully change the quality of human-AI interaction.
