# From Random to Contextual: Comparing Bandit Algorithms for News Recommendation

## Motivation

Every news website faces the same problem: a user arrives, the system has a handful of candidate articles, and it must decide which one to show. If the user clicks, that's a success. If not, an opportunity was wasted. The goal is straightforward — maximize the click-through rate (CTR) over time — but the path to getting there is not.

The core challenge is the **explore/exploit dilemma**. Should the system recommend the article it currently believes is best (exploit), or should it try something less certain to learn more about what works (explore)? Pure exploitation misses potentially great articles it has never tried. Pure exploration wastes impressions on articles that have already proven ineffective. Every recommendation system must navigate this tradeoff.

Traditional A/B testing is one approach: split users into groups, show each group a different article, and after collecting enough data, pick the winner. But A/B tests are static. They require pre-defined variants, run to completion before any learning happens, and start over from scratch with each new test. In a domain like news — where content changes hourly and user preferences shift — this rigidity is costly.

**Bandit algorithms** offer a better model. They learn in real time, continuously adapting their recommendations based on observed feedback. Instead of waiting for a test to finish, a bandit shifts traffic toward better-performing options while still exploring alternatives. The learning never stops.

Within the bandit framework there is a spectrum of sophistication. **Multi-armed bandits** (MABs) treat every user the same — they learn which articles are globally popular but cannot personalize. **Contextual bandits** go further, using information about the user and the article to make personalized decisions. A sports fan and a finance reader should see different recommendations, and contextual algorithms can learn these preferences.

In this post we compare four algorithms that span this spectrum, from a random baseline to a fully contextual approach. We evaluate them head-to-head on the Microsoft News Dataset (MIND), a collection of real user impression logs. The question: how much does each level of sophistication actually buy you?

## Methods

All four algorithms face the same decision each round: given a set of candidate articles, pick one. After the user either clicks or ignores the recommendation, the algorithm updates its internal state. The difference between algorithms is *how* they make their choice and *what* they learn from the outcome.

### RandomChoice (Baseline)

The simplest possible strategy: pick an article uniformly at random with no regard for past performance, user history, or article content.

**Strengths:** Dead simple and completely unbiased. It makes no assumptions and requires no tuning. Its only purpose is to establish a performance floor — any useful algorithm must beat this.

**Weaknesses:** It ignores everything. Every click it gets is pure luck. With ~37 candidates per round, we'd expect a CTR around 3%, but since some articles are more popular than others the actual random CTR lands around 11%.

### Epsilon-Greedy

Epsilon-greedy introduces the most basic form of learning. It tracks the average reward (click rate) for each article it has tried. With probability 1-ε it *exploits* by picking the article with the highest known average. With probability ε it *explores* by picking randomly. We use ε=0.1, meaning it exploits 90% of the time.

**Strengths:** Easy to understand and implement. It learns quickly which articles are popular across the user base. The ε parameter gives direct, intuitive control over the explore/exploit balance — turn it up for more exploration, down for more exploitation.

**Weaknesses:** Its exploration is blind. When it does explore, it picks randomly among all candidates, including ones that have already proven poor. It also treats all users identically — a sports fan and a foodie see the same recommendations. And it never stops exploring, even when it is highly confident about which articles are best. That 10% random exploration continues forever, wasting impressions on known losers.

### Thompson Sampling

Thompson Sampling takes a Bayesian approach to the explore/exploit problem. Instead of tracking a single average per article, it maintains a full probability distribution (a Beta distribution) over each article's true click rate. At decision time, it draws a random sample from each article's distribution and picks the article whose sample is highest.

**Strengths:** Exploration is principled and automatic. Articles that the algorithm is uncertain about have wide distributions — their samples sometimes come out high, naturally triggering exploration. Articles with strong evidence of high click rates have narrow distributions centered on good values, so they consistently win. There is no exploration parameter to tune. As confidence grows, exploration naturally tapers off.

**Weaknesses:** Like epsilon-greedy, Thompson Sampling is context-free. It cannot personalize. It is also sensitive to the choice of prior distribution. With over 18,000 unique articles appearing across 100,000 impressions, many articles are seen only a handful of times. A uniform prior — Beta(1,1), expressing total ignorance — can slow down learning when the arm space is large. A pessimistic prior calibrated to the base click rate helps the algorithm exploit observed clicks sooner.

**Key insight:** We use a prior of Beta(1,8), which encodes a prior belief that articles are clicked about 11% of the time (close to the actual base rate). This lets even a single click on an article meaningfully shift its posterior upward. Prior tuning is a practical consideration that textbooks often gloss over, but it matters in real-world settings with large article catalogs.

### LinUCB (Contextual Bandit)

LinUCB, introduced by Li et al. at WWW 2010, is a contextual bandit algorithm. Unlike the previous approaches, it does not treat articles or users as interchangeable. Instead, it builds a linear model for each article that predicts click probability from a **context vector** — a numerical description of the user and the article.

At decision time, LinUCB computes a predicted reward for each candidate *plus* an "optimism bonus" that is large when the algorithm is uncertain about a prediction. It picks the article with the highest optimistic estimate. This is the Upper Confidence Bound (UCB) principle: be optimistic in the face of uncertainty, because the upside of discovering a great option outweighs the cost of a bad trial.

The context vector we use is 62-dimensional, built from subcategory information rather than the coarser top-level categories. The MIND dataset has 285 subcategories — far more granular than the 18 categories. We keep the top 30 most frequent subcategories (covering 84% of articles) plus an "other" bucket, giving a 31-dimensional one-hot per article. The first 31 dimensions encode the **user profile**: a normalized subcategory-frequency vector from the user's click history. A user who clicked mostly "football_nfl" and "basketball_nba" articles would have high weight on those subcategories. The remaining 31 dimensions encode the **article subcategory** as a one-hot. This lets LinUCB learn fine-grained patterns like "NFL fans tend to click NFL articles" rather than just "sports fans like sports."

**Strengths:** LinUCB personalizes. Different users get different recommendations based on their reading history. It shares learning across contexts — a click on a sports article by a sports-oriented user informs predictions for similar user-article pairs, not just that specific article. And its exploration bonus shrinks automatically as confidence grows, so it explores less over time without requiring manual tuning.

**Weaknesses:** LinUCB is only as good as the features you give it. The algorithm itself is elegant, but feature engineering is the real bottleneck.

**Key insight:** Feature granularity matters. With only 18 top-level categories, LinUCB was learning "which categories get clicked globally" — something epsilon-greedy already captures by tracking per-article averages. Switching to 285 subcategories (bucketed into top-30 + other) and adding user subcategory profiles from click history meaningfully improved LinUCB's performance. The algorithm needs fine-grained user-article interaction signals to personalize effectively.

## Experiment

### Dataset

We evaluate on MINDlarge_train, the large training split from the Microsoft News Dataset (MIND). The full dataset contains over 2.2 million impression logs and 101,527 news articles. We sample the first 100,000 impressions for our comparison, which covers 18,866 unique articles across 18 categories including news, sports, entertainment, finance, lifestyle, health, and more. Each impression presents approximately 37 candidate articles on average, with ground-truth click labels indicating which article the user actually engaged with.

### Simulation

We replay impressions sequentially through a simulation engine. For each round, the algorithm sees the candidate articles (and, for LinUCB, the context vectors), selects one article to recommend, and then observes the ground-truth reward: 1.0 if the user clicked that article, 0.0 otherwise. The algorithm updates its internal state and moves to the next round.

Critically, all four algorithms process the same 100,000 impressions in the same order. This ensures a fair apples-to-apples comparison — differences in CTR reflect algorithmic merit, not data ordering.

### Context Construction

For LinUCB, we construct a 62-dimensional context vector per candidate article in each round. Rather than using the 18 top-level categories, we use subcategory information — the MIND dataset has 285 subcategories like "football_nfl", "newsus", and "lifestylebuzz". We keep the top 30 most frequent subcategories (covering 84% of articles) plus an "other" bucket, giving 31 dimensions. The first 31 dimensions encode the **user profile**: a normalized subcategory-frequency vector built from the articles the user has previously clicked. A user who clicked 3 NFL football and 1 NBA basketball article would have high weight on those specific subcategories. The remaining 31 dimensions encode the **article subcategory** as a one-hot vector. This concatenated representation gives LinUCB fine-grained user-article interaction signals for personalization.

### Metric

Our primary metric is click-through rate (CTR): total clicks divided by total impressions. We also track cumulative CTR at each round to visualize how algorithms learn over time.

## Results

### Final Performance

| Algorithm | CTR | vs. Random |
|---|---|---|
| RandomChoice | 0.1088 | — |
| EpsilonGreedy | 0.1931 | +77.5% |
| ThompsonSampling | 0.1821 | +67.4% |
| LinUCB | 0.1789 | +64.4% |

![Final CTR comparison across all four algorithms](../results/final_ctr.png)

All three learning algorithms substantially outperform the random baseline. Epsilon-greedy leads with a 77% relative improvement, followed by Thompson Sampling and LinUCB close behind. With 100,000 impressions, the differences between algorithms are clearly visible — EpsilonGreedy achieves 19,313 clicks compared to RandomChoice's 10,876, and LinUCB nearly matches Thompson Sampling at 17,893 clicks.

### Learning Curves

![Cumulative CTR over 100,000 impression rounds](../results/cumulative_ctr.png)

The cumulative CTR plot reveals how each algorithm learns over time. RandomChoice converges to ~10.9% and stays flat — there is no learning happening. EpsilonGreedy ramps up fastest, reaching near its final CTR by around 5,000 rounds. Thompson Sampling follows a similar trajectory but stabilizes slightly lower. LinUCB starts slower — its linear models need more data to learn the relationship between subcategory features and clicks — but steadily climbs throughout the entire run, closing the gap with Thompson Sampling by round 100,000 and still trending upward. All three learning algorithms clearly separate from the random baseline within the first few thousand rounds.

### Takeaways

**1. Any learning beats no learning.** Even the simplest learning algorithm, epsilon-greedy, achieves a 77% relative improvement over random. With 100,000 impressions, that translates to over 8,400 additional clicks. The explore/exploit framework delivers real, measurable value.

**2. Simple algorithms are hard to beat.** Epsilon-greedy — the most straightforward learning approach — outperforms both Thompson Sampling and LinUCB on this dataset. Its fast exploitation of per-article averages is highly effective when the same articles recur across many impressions. Sophistication does not automatically mean better performance.

**3. Bayesian priors matter.** Thompson Sampling performs well with a Beta(1,8) prior calibrated to the base click rate. Prior tuning is a practical consideration that textbooks often gloss over, but it matters in real-world settings with large article catalogs where many arms are sparsely observed.

**4. Feature granularity unlocks contextual algorithms.** Switching from 18 top-level categories to the top-30 subcategories meaningfully improved LinUCB's performance. Subcategories like "football_nfl" and "basketball_nba" carry far more preference signal than a flat "sports" label. LinUCB's learning curve is still climbing at 100,000 rounds, suggesting that with more data — or even richer features like title embeddings — it could surpass the context-free approaches.

**5. The framework matters more than any single result.** A clean algorithmic interface — select an arm, observe a reward, update — makes it trivial to swap in new approaches. The engineering investment in a modular simulation framework pays dividends long after any individual experiment.

### What's Next

Natural extensions include enriching features with title embeddings for semantic similarity, running on the full 2.2 million impressions to see if LinUCB's upward trajectory overtakes the context-free algorithms, adding algorithms like UCB1 and contextual Thompson Sampling, and running statistical significance tests to quantify the reliability of observed differences.
